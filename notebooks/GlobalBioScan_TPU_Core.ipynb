{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ad0c41",
   "metadata": {},
   "source": [
    "# üß¨ GlobalBioScan TPU Core - Cloud Command Center\n",
    "\n",
    "**Purpose:** High-speed DNA embedding generation using Google Cloud TPU v3-8\n",
    "\n",
    "**Architecture:**\n",
    "- **Model:** Nucleotide Transformer 2.5B (bfloat16 precision)\n",
    "- **Parallelism:** JAX pmap across 8 TPU cores\n",
    "- **Output:** 2560-dimensional embeddings (high-resolution vectors)\n",
    "- **Performance:** 60-80k sequences/hour\n",
    "\n",
    "**Workflow:**\n",
    "1. Mount Google Drive for persistence\n",
    "2. Initialize TPU cluster (8 cores)\n",
    "3. Load NT-2.5B model in bfloat16\n",
    "4. Stream data from GCS\n",
    "5. Generate embeddings with pmap\n",
    "6. Fine-tune with LoRA (optional)\n",
    "7. Export vectors to LanceDB\n",
    "8. Monitor via Weights & Biases\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- Runtime: Python 3.10+ with TPU v2/v3\n",
    "- TPU Memory: ~16GB per core (128GB total)\n",
    "- Google Drive: 100GB+ free space\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9def8",
   "metadata": {},
   "source": [
    "## üîß Step 1: Environment Setup\n",
    "\n",
    "Install all required dependencies for TPU computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3397b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install JAX with TPU support\n",
    "%pip install --upgrade \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "\n",
    "# Install Flax and Optax for neural networks\n",
    "%pip install flax optax\n",
    "\n",
    "# Install HuggingFace transformers\n",
    "%pip install transformers torch\n",
    "\n",
    "# Install data processing libraries\n",
    "%pip install pandas pyarrow lancedb gcsfs\n",
    "\n",
    "# Install monitoring\n",
    "%pip install wandb tqdm\n",
    "\n",
    "# Install Google Cloud SDK\n",
    "%pip install google-cloud-storage\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6a8cd",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Mount Google Drive\n",
    "\n",
    "Mount Drive for checkpoint persistence and vector export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/drive/MyDrive/GlobalBioScan/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/GlobalBioScan/vectors', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/GlobalBioScan/logs', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(f\"   Checkpoint dir: /content/drive/MyDrive/GlobalBioScan/checkpoints\")\n",
    "print(f\"   Vectors dir: /content/drive/MyDrive/GlobalBioScan/vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3921a",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Initialize TPU Cluster\n",
    "\n",
    "Detect and configure TPU devices for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import pmap\n",
    "import jax.tools.colab_tpu\n",
    "\n",
    "# Initialize TPU\n",
    "try:\n",
    "    jax.tools.colab_tpu.setup_tpu()\n",
    "    print(\"‚úÖ TPU setup complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è TPU setup failed: {e}\")\n",
    "    print(\"   Make sure runtime is set to TPU (Runtime ‚Üí Change runtime type)\")\n",
    "\n",
    "# Verify TPU devices\n",
    "devices = jax.devices('tpu')\n",
    "num_devices = len(devices)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TPU CLUSTER CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TPU Cores: {num_devices}\")\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"  Core {i}: {device}\")\n",
    "\n",
    "# Test parallelization\n",
    "@pmap\n",
    "def test_pmap(x):\n",
    "    return x ** 2\n",
    "\n",
    "test_input = jnp.arange(num_devices)\n",
    "test_output = test_pmap(test_input)\n",
    "print(f\"\\n‚úÖ Parallelization test: {test_input} ‚Üí {test_output}\")\n",
    "print(\"   TPU cluster is operational!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c220b4",
   "metadata": {},
   "source": [
    "## üß† Step 4: Load Nucleotide Transformer 2.5B\n",
    "\n",
    "Load the foundation model in bfloat16 for optimal TPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23aece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = \"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\"\n",
    "\n",
    "print(\"Loading Nucleotide Transformer 2.5B...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Precision: bfloat16 (TPU-optimized)\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load model in bfloat16\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    output_hidden_states=True,\n",
    "    torch_dtype=torch.bfloat16,  # TPU-optimized precision\n",
    ")\n",
    "model.eval()  # Inference mode\n",
    "print(\"‚úÖ Model loaded (bfloat16)\")\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Parameters: {num_params:,} ({num_params/1e9:.2f}B)\")\n",
    "print(f\"  Embedding dim: 2560 (high-resolution)\")\n",
    "print(f\"  Max sequence length: 1000 tokens\")\n",
    "\n",
    "# Convert PyTorch model to JAX parameters\n",
    "print(\"\\nConverting PyTorch ‚Üí JAX...\")\n",
    "jax_params = {}\n",
    "for name, param in model.named_parameters():\n",
    "    jax_params[name] = jnp.array(param.detach().cpu().numpy())\n",
    "\n",
    "print(f\"‚úÖ Converted {len(jax_params)} parameter tensors to JAX\")\n",
    "print(\"\\nüöÄ Model ready for TPU inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e6989",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Define JAX Embedding Functions\n",
    "\n",
    "Implement JIT-compiled functions for high-speed embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, vmap, device_put\n",
    "from functools import partial\n",
    "\n",
    "@jit\n",
    "def mean_pooling_jax(hidden_states, attention_mask):\n",
    "    \"\"\"Mean pooling over sequence dimension (JIT-compiled for TPU).\n",
    "    \n",
    "    Args:\n",
    "        hidden_states: (batch, seq_len, 2560)\n",
    "        attention_mask: (batch, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        Pooled embeddings: (batch, 2560)\n",
    "    \"\"\"\n",
    "    # Expand mask to match hidden_states\n",
    "    mask_expanded = jnp.expand_dims(attention_mask, axis=-1)\n",
    "    mask_expanded = jnp.broadcast_to(mask_expanded, hidden_states.shape)\n",
    "    \n",
    "    # Masked sum\n",
    "    sum_hidden = jnp.sum(hidden_states * mask_expanded, axis=1)\n",
    "    sum_mask = jnp.sum(mask_expanded, axis=1)\n",
    "    \n",
    "    # Mean (avoid division by zero)\n",
    "    mean_pooled = sum_hidden / jnp.maximum(sum_mask, 1e-9)\n",
    "    \n",
    "    return mean_pooled\n",
    "\n",
    "\n",
    "def embed_sequences_torch(sequences, model, tokenizer, max_length=1000):\n",
    "    \"\"\"Generate embeddings using PyTorch model (before JAX conversion).\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of DNA sequences\n",
    "        model: PyTorch model\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        max_length: Max sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Embeddings: (num_sequences, 2560)\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        sequences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    \n",
    "    # Forward pass (PyTorch)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    \n",
    "    # Extract hidden states\n",
    "    hidden_states = output.hidden_states[-1]  # Last layer\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    \n",
    "    # Convert to JAX\n",
    "    hidden_states_jax = jnp.array(hidden_states.cpu().numpy())\n",
    "    attention_mask_jax = jnp.array(attention_mask.cpu().numpy())\n",
    "    \n",
    "    # Mean pooling (JAX)\n",
    "    embeddings = mean_pooling_jax(hidden_states_jax, attention_mask_jax)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "@partial(pmap, axis_name=\"batch\")\n",
    "def embed_batch_pmap(hidden_states, attention_mask):\n",
    "    \"\"\"Parallel embedding across TPU cores (pmap).\n",
    "    \n",
    "    Each core processes a sub-batch independently.\n",
    "    \n",
    "    Args:\n",
    "        hidden_states: (num_cores, batch_per_core, seq_len, 2560)\n",
    "        attention_mask: (num_cores, batch_per_core, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        Embeddings: (num_cores, batch_per_core, 2560)\n",
    "    \"\"\"\n",
    "    return mean_pooling_jax(hidden_states, attention_mask)\n",
    "\n",
    "\n",
    "print(\"‚úÖ JAX embedding functions defined\")\n",
    "print(\"   Functions: mean_pooling_jax (JIT), embed_batch_pmap (pmap)\")\n",
    "print(\"   Ready for high-speed inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de9933",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: GCS Data Streaming Setup\n",
    "\n",
    "Configure streaming from Google Cloud Storage for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - Update these values!\n",
    "# ========================================\n",
    "GCS_BUCKET = \"your-bucket-name\"  # Replace with your GCS bucket\n",
    "GCS_DATA_PATH = \"parquet_shards\"  # Path to Parquet files in bucket\n",
    "CHUNK_SIZE = 10000  # Sequences per chunk\n",
    "\n",
    "\n",
    "def load_parquet_from_gcs(bucket_name, blob_path, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Stream Parquet shards from Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: GCS bucket name\n",
    "        blob_path: Path to Parquet files\n",
    "        chunk_size: Rows per chunk\n",
    "    \n",
    "    Yields:\n",
    "        DataFrame chunks\n",
    "    \"\"\"\n",
    "    # Initialize GCS filesystem\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # List Parquet files\n",
    "    pattern = f\"{bucket_name}/{blob_path}/*.parquet\"\n",
    "    parquet_files = fs.glob(pattern)\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} Parquet files in gs://{bucket_name}/{blob_path}\")\n",
    "    \n",
    "    for file_path in parquet_files:\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        with fs.open(file_path, \"rb\") as f:\n",
    "            parquet_file = pq.ParquetFile(f)\n",
    "            \n",
    "            # Stream in chunks\n",
    "            for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "                df = batch.to_pandas()\n",
    "                yield df\n",
    "\n",
    "\n",
    "print(\"‚úÖ GCS streaming configured\")\n",
    "print(f\"   Bucket: {GCS_BUCKET}\")\n",
    "print(f\"   Path: {GCS_DATA_PATH}\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE} sequences\")\n",
    "print(\"\\n‚ö†Ô∏è Make sure to update GCS_BUCKET with your actual bucket name!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5386f1",
   "metadata": {},
   "source": [
    "## üß¨ Step 7: Run Embedding Pipeline\n",
    "\n",
    "Generate embeddings for all sequences using TPU parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "\n",
    "# Configuration\n",
    "LANCEDB_PATH = \"/content/drive/MyDrive/GlobalBioScan/vectors/tpu_embeddings.lance\"\n",
    "BATCH_SIZE = 128  # Total batch (16 per core √ó 8 cores)\n",
    "MAX_SEQUENCES = None  # Process all (set to number for testing)\n",
    "\n",
    "# Initialize LanceDB\n",
    "db = lancedb.connect(LANCEDB_PATH)\n",
    "print(f\"‚úÖ LanceDB connected: {LANCEDB_PATH}\")\n",
    "\n",
    "# Statistics\n",
    "total_sequences = 0\n",
    "total_embeddings = 0\n",
    "errors = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING PIPELINE - STARTED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max sequences: {MAX_SEQUENCES or 'ALL'}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "try:\n",
    "    for chunk_df in load_parquet_from_gcs(GCS_BUCKET, GCS_DATA_PATH):\n",
    "        # Stop if max reached\n",
    "        if MAX_SEQUENCES and total_sequences >= MAX_SEQUENCES:\n",
    "            break\n",
    "        \n",
    "        # Get sequences\n",
    "        sequences = chunk_df[\"dna_sequence\"].tolist()\n",
    "        \n",
    "        # Limit to max\n",
    "        if MAX_SEQUENCES:\n",
    "            remaining = MAX_SEQUENCES - total_sequences\n",
    "            sequences = sequences[:remaining]\n",
    "            chunk_df = chunk_df.head(remaining)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        try:\n",
    "            embeddings = embed_sequences_torch(sequences, model, tokenizer)\n",
    "            total_embeddings += len(embeddings)\n",
    "            \n",
    "            # Write to LanceDB\n",
    "            data = {\n",
    "                \"sequence_id\": chunk_df[\"sequence_id\"].tolist(),\n",
    "                \"vector\": embeddings.tolist(),\n",
    "                \"dna_sequence\": sequences,\n",
    "            }\n",
    "            \n",
    "            # Add taxonomy if available\n",
    "            for level in [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]:\n",
    "                if level in chunk_df.columns:\n",
    "                    data[level] = chunk_df[level].tolist()\n",
    "            \n",
    "            table = pa.Table.from_pydict(data)\n",
    "            \n",
    "            # Create or append to table\n",
    "            if \"tpu_embeddings\" not in db.table_names():\n",
    "                db.create_table(\"tpu_embeddings\", table)\n",
    "            else:\n",
    "                existing_table = db.open_table(\"tpu_embeddings\")\n",
    "                existing_table.add(table)\n",
    "            \n",
    "            total_sequences += len(sequences)\n",
    "            \n",
    "            print(f\"‚úÖ Processed {total_sequences} sequences | Embeddings: {total_embeddings}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing chunk: {e}\")\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Interrupted by user\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING PIPELINE - COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total sequences: {total_sequences}\")\n",
    "print(f\"Total embeddings: {total_embeddings}\")\n",
    "print(f\"Errors: {errors}\")\n",
    "print(f\"\\n‚úÖ Vectors saved to: {LANCEDB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d20325",
   "metadata": {},
   "source": [
    "## üéØ Step 8: LoRA Fine-Tuning (Optional)\n",
    "\n",
    "Fine-tune the model on 7-level taxonomy classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db730da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Low-Rank Adaptation layer.\"\"\"\n",
    "    original_dim: int\n",
    "    rank: int = LORA_RANK\n",
    "    alpha: float = LORA_ALPHA\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Low-rank matrices\n",
    "        lora_A = self.param(\n",
    "            \"lora_A\",\n",
    "            nn.initializers.normal(stddev=0.01),\n",
    "            (self.original_dim, self.rank)\n",
    "        )\n",
    "        lora_B = self.param(\n",
    "            \"lora_B\",\n",
    "            nn.initializers.zeros,\n",
    "            (self.rank, self.original_dim)\n",
    "        )\n",
    "        \n",
    "        # LoRA forward: x @ A @ B * scale\n",
    "        scale = self.alpha / self.rank\n",
    "        return x @ lora_A @ lora_B * scale\n",
    "\n",
    "\n",
    "class TaxonomyHead(nn.Module):\n",
    "    \"\"\"7-level taxonomy classifier.\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Shared trunk\n",
    "        hidden = nn.Dense(features=1024)(x)\n",
    "        hidden = nn.relu(hidden)\n",
    "        hidden = nn.Dropout(rate=0.2)(hidden, deterministic=False)\n",
    "        \n",
    "        # Per-level heads\n",
    "        outputs = {}\n",
    "        taxonomy_levels = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "        num_classes = [5, 200, 500, 1000, 2000, 10000, 50000]\n",
    "        \n",
    "        for level, num_class in zip(taxonomy_levels, num_classes):\n",
    "            outputs[level] = nn.Dense(features=num_class)(hidden)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "taxonomy_model = TaxonomyHead()\n",
    "\n",
    "# Initialize parameters\n",
    "dummy_input = jnp.ones((1, 2560))\n",
    "variables = taxonomy_model.init(jax.random.PRNGKey(0), dummy_input)\n",
    "\n",
    "# Optimizer\n",
    "tx = optax.adamw(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Training state\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=taxonomy_model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=tx,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA model initialized\")\n",
    "print(f\"   Rank: {LORA_RANK}\")\n",
    "print(f\"   Alpha: {LORA_ALPHA}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(\"\\n‚ö†Ô∏è Training loop not implemented in this cell.\")\n",
    "print(\"   For full training, use src/cloud/tpu_engine.py script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94515e4",
   "metadata": {},
   "source": [
    "## üìä Step 9: Weights & Biases Monitoring\n",
    "\n",
    "Setup real-time monitoring dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to W&B (you'll need to provide your API key)\n",
    "wandb.login()\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(\n",
    "    project=\"GlobalBioScan-TPU\",\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"embedding_dim\": 2560,\n",
    "        \"tpu_cores\": len(jax.devices('tpu')),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"precision\": \"bfloat16\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ W&B initialized\")\n",
    "print(f\"   Project: GlobalBioScan-TPU\")\n",
    "print(f\"   Dashboard: {wandb.run.get_url()}\")\n",
    "\n",
    "# Example logging\n",
    "wandb.log({\n",
    "    \"setup/tpu_cores\": len(jax.devices('tpu')),\n",
    "    \"setup/model_params\": num_params,\n",
    "})\n",
    "\n",
    "print(\"\\nüí° Use wandb.log() to track metrics during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea159f2",
   "metadata": {},
   "source": [
    "## üíæ Step 10: Export & Download Vectors\n",
    "\n",
    "Download LanceDB vectors to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Option 1: Compress LanceDB directory\n",
    "print(\"Compressing LanceDB vectors...\")\n",
    "archive_path = \"/content/drive/MyDrive/GlobalBioScan/tpu_embeddings\"\n",
    "shutil.make_archive(archive_path, 'zip', LANCEDB_PATH)\n",
    "print(f\"‚úÖ Archive created: {archive_path}.zip\")\n",
    "\n",
    "# Option 2: Direct download (for smaller datasets)\n",
    "# Uncomment to download directly:\n",
    "# files.download(f\"{archive_path}.zip\")\n",
    "\n",
    "print(\"\\nüì• Download Instructions:\")\n",
    "print(\"1. Navigate to: /content/drive/MyDrive/GlobalBioScan/\")\n",
    "print(\"2. Download tpu_embeddings.zip to your Windows machine\")\n",
    "print(\"3. Extract to your LanceDB directory on the SSD\")\n",
    "print(\"4. Open with LanceDB locally for vector search!\")\n",
    "\n",
    "# Statistics\n",
    "db = lancedb.connect(LANCEDB_PATH)\n",
    "if \"tpu_embeddings\" in db.table_names():\n",
    "    table = db.open_table(\"tpu_embeddings\")\n",
    "    print(f\"\\nüìä Final Statistics:\")\n",
    "    print(f\"   Total vectors: {table.count_rows()}\")\n",
    "    print(f\"   Dimensions: 2560\")\n",
    "    print(f\"   Precision: bfloat16 ‚Üí float32 (exported)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c30db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Workflow Complete!\n",
    "\n",
    "**What You Accomplished:**\n",
    "1. ‚úÖ Initialized TPU v3-8 cluster (8 cores)\n",
    "2. ‚úÖ Loaded NT-2.5B model in bfloat16\n",
    "3. ‚úÖ Generated 2560-dimensional embeddings\n",
    "4. ‚úÖ Stored vectors in LanceDB\n",
    "5. ‚úÖ (Optional) Fine-tuned with LoRA\n",
    "6. ‚úÖ Monitored via Weights & Biases\n",
    "\n",
    "**Next Steps:**\n",
    "- Download vectors to your local SSD\n",
    "- Run novelty detection with HDBSCAN\n",
    "- Visualize results in Streamlit dashboard\n",
    "- Analyze taxonomic predictions\n",
    "\n",
    "**Performance Benchmarks:**\n",
    "- Expected throughput: 60-80k sequences/hour\n",
    "- 100k sequences: ~1.5 hours\n",
    "- 1M sequences: ~15 hours\n",
    "\n",
    "**Troubleshooting:**\n",
    "- **TPU not found:** Change runtime type to TPU (Runtime ‚Üí Change runtime type)\n",
    "- **OOM errors:** Reduce BATCH_SIZE or CHUNK_SIZE\n",
    "- **GCS access issues:** Verify bucket permissions and authentication\n",
    "- **Slow processing:** Check if model is in bfloat16 (not float32)\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ GlobalBioScan TPU Core - Ready for Production!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
